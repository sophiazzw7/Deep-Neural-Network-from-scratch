 Was saying that MC A was submitted to wrong email bugs. And uh so, but now it's corrected and it's in progress. So we need to reflect the fact that we recognize the need to rate uh dual score matrices as models. OK. OK. OK. So I'll mention that we have submitted MC A um for dual metrics and we are aware that they have to be identified as model in my validation report. Is that correct on the standing Alex? That that's correct. That's correct. So it's, it's not a finding, but we need to highlight that fact.


### Meeting Notes

**Subject**: Discussion on Historical Sales Data and Model Fit-for-Use

**Participants**: 
- Alex
- Other Team Members

**Key Points Discussed**:

1. **MC A Submission and Dual Score Matrices**:
   - Corrected initial error in submission email.
   - Acknowledgment of the need to rate dual score matrices as models in the validation report.
   - Highlight the necessity without it being considered a finding.

2. **Sales Strategy Post-Merger**:
   - Historical BB&T and SunTrust branches were selling live stream products.
   - Post-merger, live stream products were pulled from branches to avoid cannibalizing business.
   - Historical SunTrust clients can still apply for live stream products online.

3. **Current Sales Channels**:
   - Transition from branch sales to digital channels and call centers.
   - Current in-branch sales focus on secured loan products.
   - Future plan to reintroduce live stream products in branches, replacing current unsecured loans.

4. **Fit-for-Use Evaluation**:
   - Expansion in the user population requires fit-for-use evaluation.
   - The fit-for-use review will be necessary before launching the email campaign.
   - Timeline for system completion by end of the year, with sales starting next year.

5. **Performance Metrics and Data Evaluation**:
   - Use of OGM (Operational Gross Margin) data to evaluate performance.
   - Review of risk and limitations, model use, and assumptions.
   - Additional testing on OGM data, variable selections, and performance metrics.

6. **Data Certification**:
   - Discussed the need for data certification, especially for external data from TransUnion.
   - Certification could involve multiple forms and examples will be shared for clarification.
   - Ensuring data integrity for both internal and external data sets.

7. **Action Items**:
   - Submit MC A form and perform testing on DE model.
   - Evaluate model assumptions and performance using OGM data.
   - Review raw data and OGM codes.
   - Obtain examples of data certification for internal review.

8. **Additional Points**:
   - Ensuring badge statuses are tracked for all team members.
   - Potential parallel processing for green card advertisement and labor certification.

**Next Steps**:
- Alex to submit the necessary forms and perform the required testing.
- Review the OGM data and provide performance metrics.
- Gather and review examples of data certification.
- Keep track of any updates in policies and procedures.

**Thank you for the productive discussion and cooperation. Please let me know if there are any further questions or details needed.**

---

### Meeting Notes

**Subject**: Discussion on TransUnion Data Verification and Certification

**Participants**: 
- Alex
- Other Team Members

**Key Points Discussed**:

1. **Verification of Application Data**:
   - Emphasis on verifying the overall distribution of acquisition quarters, number of applications, and PII format.
   - Certification of internal data used to send to TransUnion for integrity evidence.

2. **Clarification on Certification Process**:
   - Lack of a clear definition of certification within the team.
   - Focus on tracing the custody of data and ensuring data quality at each transfer point.
   - Aim to document the verification and data flow processes.

3. **Data Integrity Checks**:
   - Expectation of reasonable checks for data quality, acknowledging that 100% match is not expected.
   - Manual involvement in the process, with ultimate responsibility for data integrity lying with the team.

4. **Data Handling by TransUnion and Zest**:
   - Trade line data sent directly from TransUnion to Zest without the team touching this part of the data.
   - Discussion on the possibility of getting verification or data shape checks from TransUnion to ensure data completeness.

5. **Action Items**:
   - Document the verification process and data flow performed by the team.
   - Request TransUnion for any data shape checks or distribution verification they perform upon receiving data.

6. **Additional Points**:
   - Importance of providing thorough documentation to get credit for the work done.
   - Ensuring data integrity across all data sources and maintaining evidence for it.

**Next Steps**:
- Alex to document the data verification process and share it with the team.
- Follow up with TransUnion for additional data verification evidence.
- Continue to trace and ensure data integrity throughout the data transfer process.

**Thank you for the productive discussion and cooperation. Please let me know if there are any further questions or details needed.**

---

### Meeting Notes

**Subject**: Data Verification and Certification Process

**Participants**: 
- Vera
- Other Team Members

**Key Points Discussed**:

1. **Data Comparison**:
   - Differences between input and output files expected.
   - Need for comparison between requested and received data.
   - Matching archives for each acquisition vintage to identify acceptable differences.

2. **Documentation Requirement**:
   - Document the data requested vs. received.
   - Provide reasons for any missing data and justify why it's acceptable.
   - Example: matching data with rationales for non-matches.

3. **Understanding Data Flow**:
   - TransUnion sends data directly to Zest without team involvement.
   - Need to clarify data flow and match checkpoints.
   - Create a detailed data process flow with checkpoints for verification.

4. **Matching and Verification**:
   - Importance of matching data counts and ensuring data quality.
   - Check data received by Zest against what was sent by TransUnion.
   - Update data flow documentation to reflect the process accurately.

5. **Handling OGM Data**:
   - Certification needed for both development data and OGM data sources.
   - OGM data as real-time production data requires a nuanced approach.
   - Focus on ensuring input data accuracy for real-time scoring by Zest.

6. **Next Steps**:
   - Vera to create a detailed data flow with checkpoints and rationales for non-matches.
   - Follow up on how to handle certification for real-time production data.
   - Vera and team members to update documentation and provide further details next week.

**Action Items**:
- **Vera**: Document data verification process and share updated data flow.
- **Team**: Ensure all data sources are checked and documented for integrity.
- **Follow Up**: Investigate and find best practices for certifying real-time production data.

**Conclusion**:
- Agreement on the next steps and action items.
- Acknowledgment of the need for thorough documentation and data integrity checks.

**Meeting Adjourned**.


**Meeting Notes**

**Date:** [Insert Date]

**Participants:** [List of Participants]

**Subject:** Discussion on Integration of Live Stream Product Post-Merger

---

**Context and Background**
- The discussion revolved around the integration of the live stream product post-merger between BB&T and SoundTrust.
- Historically, both SunTrust and SoundTrust branches had the capability to sell live stream products in-branch. However, post-merger, this capability was restricted to online channels only due to the introduction of BB&T's unsecured product line.

---

**Key Points Discussed**

1. **Product Integration and Strategy**
   - The integration strategy focuses on continuing the campaign for historical SoundTrust clients until the end of 2021.
   - There is a goal to grow the live stream product line to counter the current portfolio shrinkage.

2. **Channel Strategy**
   - The shift from in-branch to online channels for selling live stream products.
   - Plans for a co-branded online presence with Choose, emphasizing digital channels for future growth.

3. **Fit For Use Assessment**
   - A fit for use assessment is required before integrating the new population into the existing model.
   - This assessment will evaluate the model's suitability for the expanded user group.

4. **Marketing Campaign**
   - Plans for an email campaign targeting clients unable to fulfill in-branch applications, set for Q3/Q4 this year.

5. **Data Certification**
   - Discussion on the need for data certification, particularly concerning external data from TransUnion.
   - Clarification on the handling of performance labels and ensuring consistency across models.

---

**Next Steps**
- Conduct a fit for use assessment before proceeding with the email marketing campaign.
- Review examples of acceptable data certifications and determine the appropriate certification process.
- Further discussions on model validation and ongoing integration processes.

---

**Action Items**
- [Action Item 1]: Conduct fit for use assessment by [Deadline].
- [Action Item 2]: Review data certification examples and propose a certification process by [Deadline].
- [Action Item 3]: Coordinate with validator regarding ongoing model validation and integration updates.

---

**Conclusion**
The meeting provided clarity on the strategy for integrating live stream products post-merger and highlighted key steps and considerations moving forward. Further actions and discussions are planned to ensure a smooth integration process and compliance with certification requirements.

---

**Next Meeting:** [Scheduled Date and Time]

---

[End of Meeting Notes]



**Meeting Notes**

**Date:** [Insert Date]

**Participants:** [List of Participants]

---

**Participant 1:**
"Yeah. OK. So, but there is gonna be a account differences between the input file versus output file, right?"

**Participant 2:**
"Yeah, I think that that is that, that is my question because um we, we wanted um a comparison between like what we requested versus what we got."

**Participant 1:**
"And if so the top one is what, what you, what we requested, right? So the bottom one is what we got because we match two different archives for each acquisition vintage."

**Participant 2:**
"You will have one archive at a time uh of March 2020 16. You have another archive at the time of June 2020. So there is always gonna be a differences between those two, right? But as long as the difference is is smaller enough, I feel we feel fine then that's usually where we're gonna go."

**Participant 1:**
"Yeah. Could you could you um could you document all that? Because what we wanted to see is like like what is the data you requested versus the data you got? And why is there any missing? If yes, um why do you think that's OK. Things like that."

**Participant 2:**
"Oh OK. So I have some kind of uh like reason about this. Yeah. Yeah. Try to provide a bit of uh provide a bit of um um like rational here."

**Participant 1:**
"Um Here u usually what, what we do is um we do like matching or not. Um You will have another, for example, just a small example, you will have a matching or not like match and you will have some yes and some nos and if there's a no, there's a rational here. And why do you think that's a acceptable things like that?"

**Participant 2:**
"Um Yeah. Um Yeah, one suggestion is to provide a bit of uh provide a bit of um um like rational here."

**Participant 1:**
"And also I wanted to um ask you regarding the Third Excel here. I I was trying to match things up with um the second one but I am having a bit of hard time here reading this Third Excel."

**Participant 2:**
"Is this the Excel that um you share the data with? DAS?"

**Participant 1:**
"No. So I I want to explain it in detail, right? So the, the Z data is actually sent out directly from Transunion, it then go to us. So this is the detailed data which Transunion send us to Z directly."

**Participant 2:**
"So what we are trying to match is look at this data and see if it's aligned with, you know what we are seeing."

**Participant 1:**
"Yeah, that would be great. That would be great because I, I was trying to figure out like where all those pieces come together because what, what, what we are trying to see is how the data actually flows and how at each step there's a match or there's a check things like that."

**Participant 2:**
"Um OK. Yeah, let me create a flow data flow. So um yeah, that would be great. Maybe get some checkpoints on that string, application data. I can actually use the data process flow but add some checkpoints on that page."

**Participant 1:**
"OK. Let me do that. Yeah, maybe try to explain a bit more like how these numbers matching with like the second one, things like that just to get a bit of um details on it because right now it's, it's pretty difficult to follow like how the data flow from Transunion to that."

**Participant 2:**
"For example, if you are, I'm assuming you have two copies, one transunion directly sent to you and Transunion directly sent to that. If you do some tracks between those, what are like the um you probably compare the counts. Um What is like the checks or the um quality assurance things that you do."

**Participant 1:**
"These are some examples um that I could think of. Also, I know we talked about the reject influence data that was directly sent to um Z as well from Transunion."

**Participant 2:**
"Um Could you also reject, what do you mean reject data?"

**Participant 1:**
"So here, let me go back to this one. So here there was a data, it's, it was."

**Participant 2:**
"So it's a target. So it's a target. So there the fire from um I don't know if this one explain well enough."

**Participant 1:**
"So the starting point is what we send all the input file to Transunion Transunion append their TU data set, then they send back to us and they will send the one copy to um that we're on the train line data, right? So it, it's not the same data that Transunion share with you and versus they share with us. So it's the same data they share with us versus Z. But the only thing is that get is deducted data, meaning they do not get, they only get the trade line data, they don't get other information, they only get the trade line data at the time of acquisition. They don't get the performance data."

**Participant 2:**
"That's right. So what we actually create, which is the same as that L model is what Luis created the target file. Then we send that target file back to, to Z as their, you know, every, every 60 within our 1215, 18 months, that's the file we send to Z, then they match to that whatever the train line that they get."

**Participant 1:**
"Oh, got it. Can we um maybe clarify a little bit about that because looking at this, it's not the, the process is, well, it was a little different from what I heard from you."

**Participant 2:**
"So um and maybe try to, I would say try to um update this graph a little bit to include it in the MD D um just to clarify things up a little."

**Participant 1:**
"Um And I would so for here, all three excels are internal here, right? We don't get anything from that."

**Participant 2:**
"Um We don't know if there's anything they done to check the cons or check if the data they receive is complete."

**Participant 1:**
"This is, this is the count, the quantity is actually coming from uh what Trans Union send out to that."

**Participant 2:**
"I get the count. I don't get the detailed data."

**Participant 1:**
"OK. So this is the cons that that received. Is that right?"

**Participant 2:**
"OK. OK. Let me take a look and get back to you because I do have to take back to my original data and make sure I get everything."

**Participant 1:**
"Yeah, sure that'd be good."

**Participant 2:**
"Um I also wanted to uh do like a a remind you really quick about the uh because I see the data was 2 2020 um here."

**Participant 1:**
"So I do want to make sure that we are also doing the same thing for the OGN data source as well."

**Participant 2:**
"What do you mean doing the same?"

**Participant 1:**
"So they there are two piece of certification is development data and the OGM data source. So whatever we are using in the OGM reports like the the the OGM data, they also would need to be checked."

**Participant 2:**
"Oh OK. So that's gonna be a little bit nuance on that. Um I can create a waterfall on that piece. But the problem is OGM data is a real production data, right?"

**Participant 1:**
"Um So it's basically what zest actually produce at real time that might have to."

**Participant 2:**
"So that's external data. None of those."

**Participant 1:**
"Yeah, the internal is over the performance data we send over. Um But the rest of data is actually at the Z how how does that piece actually work out? How are we going to certify it on that?"

**Participant 2:**
"So the application, so essentially the internal piece is the performance data we provided for comparison, right? And the application scoring happened on that."

**Participant 1:**
"Basically there's an internal label, we we provide it and there's an external label they they done and we do a comparison that's producing the OGM report. Is that, is that right?"

**Participant 2:**
"Um The performance data, yes, we provided the the the rest is production real time because they are scoring data for us, right? So that's what they are providing, it's that they are getting um real time data from us, which is the builder data. They create a score right on the spot."

**Participant 1:**
"Yep. And that score is actually the output. So we only care about the input part. So we want to make sure everything we sent to for them to for them to score is accurate."

**Participant 2:**
"Yeah, yeah, that piece uh we don't, we don't check for here because there's no way to check it, right? Because we do. But again, it's production Right. So it's production data, how it's all, it's real time, everything is systematic."

**Participant 1:**
"I mean, how are we gonna do this? So there's no, it's all like we have a, like a,

 like number of cons or things like that. Right."

**Participant 2:**
"Yeah, I, I got it. What's the production? Do you have any, like, because all the model usually majority of the model run real time. Right."

**Participant 1:**
"So, do you have any things I can follow?"

**Participant 2:**
"Yeah, I'm going to try to dig more into this. Um That is a great question. Uh So I will try to find out more about how do we deal with like real time production data? How do we do certification on that? I don't know this answer yet, but I will get back to you once I ok. So give me a little bit of time because I will be out for the last two days of this week. I probably have to get back to you when I come back next week."

**Participant 1:**
"That's fine. That's totally fine. I'll be, I'll be out as well, so no worries."

**Participant 2:**
"Ok, have a great long weekend then."

**Participant 1:**
"Ok, thank you. Thank you, Vera. Bye bye."

**Participant 2:**
"Ok. Yeah. Yeah."

---

[End of Meeting Notes]


