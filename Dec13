Same Event ID, Same Posting Date, Same Gross Loss & Recovery Amounts
Example: IE-12097866 on 02/27/2019 appears multiple times, each with:
Gross Loss = $88,415.00
Recovery = ($2,415.00)
Net Loss = 86,000.00
GL Account = 860340 (same for all)
The only difference is GL Tracking ID, but that might just be a system artifact.
This looks like a duplicate rather than a true separate transaction.
Same Event ID, Same Posting Date, Different Gross/Net Loss
Example: IE-12097912 on 08/29/2019
First row: Gross Loss = $558,000.00, Recovery = ($558,000.00), Net Loss = $0
Second row: Identical
These are exact duplicates and should be removed before summation.
Different Gross Loss Amounts per Same Event & Date
Example: IE-12097830 on 10/01/2019
Gross Loss = $3,086,682.02 (repeated)
Net Loss = $3,086,682.02
Different GL Tracking IDs
Possible explanation: If this is a duplicate, it could double your severity sum.
Action: Check the source system to see if this is an actual correction or a repeated data entry error.



### **Subject:** Clarification on Duplicate Handling in `event_w_exclusion` vs. `event_w_exclusion_dedup`  

Hi [Developer's Name],  

I was reviewing the duplicate handling process in the data pipeline, specifically comparing `event_w_exclusion` and `event_w_exclusion_dedup`. Below are my observations:  

### **Observations on `event_w_exclusion`**
- I found cases where the **same `Internal Events` appear multiple times** with identical values for:
  - `Posting Date`
  - `Gross Loss Amount`
  - `Recovery Amount`
  - `net_loss`
  - `GL Account`
- The only difference among these duplicate rows is the **GL Tracking ID**.
- This suggests that multiple G/L entries for the same event exist in `event_w_exclusion`, which could impact **severity calculations** if they are not meant to be separate transactions.

### **Observations on `event_w_exclusion_dedup`**
- When I checked `event_w_exclusion_dedup`, it returned **no duplicate records** based on `Internal Events` and `net_loss`.
- This is likely because `event_w_exclusion_dedup` is created using a `SELECT DISTINCT` on:
  - `'Basel Event Type Level 1'n`
  - `year_event`
  - `qtr_event`
  - `gl_date`
  - `'Event Record Type'n`
  - `'Internal Events'n`
  - `net_loss`
- Since **GL Tracking ID and Posting Date were not included**, rows that were distinct in `event_w_exclusion` may have been collapsed into one event record in `event_w_exclusion_dedup`.

### **Question for Clarification**
- Was this **intentional behavior** to aggregate multiple G/L entries into a single event record in `event_w_exclusion_dedup`?  
- Or should `event_w_exclusion_dedup` be preserving **all distinct records from `event_w_exclusion`**, including variations in **GL Tracking ID and Posting Date**?  
- If the goal was to sum all distinct G/L entries per event before rolling up to `event_w_exclusion_dedup`, should a different aggregation approach be used before deduplication?  

I've attached the **duplicates detail from `event_w_exclusion`** for reference. Let me know your thoughts on this!  

Best,  
[Your Name]  

---

### **Next Steps**
Let me know if you'd like any adjustments to the message or additional attachments before sending it to the developer! ðŸš€




==========


Below is a SAS code snippet that checks for duplicates in **event_w_exclusion_dedup** based on the combination of `'Internal Events'n` and `net_loss`. If no rows are returned from this query, then there are no duplicates according to those key fields.

---

```sas
/* Check for duplicates in event_w_exclusion_dedup */
proc sql;
    create table dup_check as
    select 
        'Internal Events'n as event_id,
        net_loss,
        count(*) as count_occurrences
    from event_w_exclusion_dedup
    group by 
        'Internal Events'n, net_loss
    having count(*) > 1;
quit;

proc print data=dup_check;
run;
```

---

### **Summary of Observations**

**Regarding `event_w_exclusion`:**
- We observed that in the **event_w_exclusion** dataset there are multiple rows for the same event (i.e., same `'Internal Events'n`, same `Posting Date`, same `Gross Loss Amount`, `Recovery Amount`, `net_loss`, and `GL Account`), differing only in the **GL Tracking ID**.
- This indicates that multiple G/L entries exist for a single event.

**Regarding `event_w_exclusion_dedup`:**
- This dataset is created using a `SELECT DISTINCT` on a limited set of fields:
  - `'Basel Event Type Level 1'n`, `year_event`, `qtr_event`, `gl_date`, `'Event Record Type'n`, `'Internal Events'n`, and `net_loss`.
- As a result, differences in fields such as **Posting Date** and **GL Tracking ID** are not considered.
- Our duplicate check (using the code above) based on `'Internal Events'n` and `net_loss` helps us determine if the deduplication collapsed rows as intended.
- If duplicates (i.e., same event ID and net loss) still appear, that implies multiple records with identical event-level details exist in **event_w_exclusion_dedup**â€”either because they are legitimate partial postings or because the deduplication did not remove pure duplicates.

---

### **Email to Developer**

Subject: **Clarification on Duplicate Handling in event_w_exclusion vs. event_w_exclusion_dedup**

Hi [Developer's Name],

Iâ€™ve been reviewing the duplicate handling logic in our data pipeline. Hereâ€™s a summary of my observations:

1. **event_w_exclusion:**  
   - I found that some events appear multiple times with the same values for Posting Date, Gross Loss Amount, Recovery Amount, net_loss, and GL Accountâ€”differing only by the GL Tracking ID.
   - This suggests that multiple G/L entries exist for a single event.

2. **event_w_exclusion_dedup:**  
   - This dataset is built using a SELECT DISTINCT on key fields (Basel Event Type Level 1, year_event, qtr_event, gl_date, Event Record Type, Internal Events, and net_loss).  
   - As a result, fields like Posting Date and GL Tracking ID are not included in the deduplication logic.
   - I ran a duplicate check (see attached SAS code below) on event_w_exclusion_dedup based on Internal Events and net_loss.  
   - The duplicate check reveals that some duplicate records still exist based on these keys.

Could you please confirm if this behavior is intended? Specifically, should we be collapsing multiple G/L entries for the same event (even if they differ only in GL Tracking ID), or should we preserve these differences further upstream?

For your reference, Iâ€™ve attached the code used for the duplicate check.

Thank you,  
[Your Name]

---

Please let me know if any further adjustments are needed!
