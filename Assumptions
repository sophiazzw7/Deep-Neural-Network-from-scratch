### **4. Review of Data Quality and Data Appropriateness**

This section evaluates the quality and appropriateness of the data used in the model development process, focusing on data sources, relevance, and processing procedures.

#### **Data Sources Appropriateness**
The data sources include historical transactions, sales, and service data derived from the BB&T PC Teller System, BST Enterprise Teller System, BB&T Client Central, BST CLP, and EdgeSell. These data sources are relevant as they directly track client-facing activities, such as teller transactions, platform sales, and service activities, which are essential for generating accurate forecasts. The inclusion of both client-facing and non-client-facing activities ensures a comprehensive approach to workload forecasting.

#### **Data Quality and Relevance**
The historical data used spans 24 months, providing a sufficient period for capturing client behavior and trends. The data from the BST Enterprise Teller System and other platforms are considered a strong indicator of future client behavior, making them highly relevant to the forecast. The updates, such as the cash tier update in Fall 2021, ensure that the data remains current and aligned with operational changes.

#### **Data Representativeness**
The data sources are based on actual branch activities, reflecting the unique transaction and service mixes at each branch. This ensures that the data is representative of the specific operational environment at each branch and the wider network. Non-client-facing activities are also included, reflecting both industry-standard activities and those defined by the Truist Leadership Model (TLM).

#### **Data Extraction, Cleaning, and Smoothing Procedures**
Data is extracted from the relevant systems, with historical data feeds being validated. For example, teller transactions are mapped into forecast activity groups, and unnecessary data, such as night deposit transactions, is excluded. Each type of transaction and activity is assigned a time standard, ensuring consistency across the data.

#### **Data Integrity**
The historical data feeds from systems such as the mainframe and BST systems have been validated to ensure accuracy and appropriateness. Kiran has verified that all time standards and activities align with industry norms, supporting the data's reliability for model forecasting.

### **4.1 Summary of Data Processing**

This section provides a summary of the process used to develop the model dataset, documenting the steps taken and identifying any gaps or missing information.

#### **Data Sources and Extraction**
Data sources included:
- **BB&T PC Teller System** and **BST Enterprise Teller System** for teller transactions.
- **BB&T Client Central**, **BST CLP**, and **EdgeSell** for platform sales and service data.

These data sources were confirmed to capture relevant transactional and client-facing activities. However, there is no specific mention of whether a **comprehensive data extraction process** was documented, which could impact transparency in the data source validation.

#### **Data Cleaning and Transformation Techniques**
- **Cleaning**: The data was cleaned to remove irrelevant transactions (e.g., night deposits).
- **Transformation**: Transactions were mapped to forecast groups and time standard groups. This process was described at a high level, but there is **no detailed documentation of empirical support** for the data cleaning or backfilling techniques used. The exact cleaning criteria and rationale are not provided.

#### **Variable Creation and Data Dictionary**
A **data dictionary** was required, but no explicit mention of a formal data dictionary has been provided. Essential elements such as:
- Descriptions of model variables.
- Variable creation rules and source fields.
- Formats (units) for each variable.
- Decoding rules for categorical variables.
- Basic descriptive statistics.

These appear to be missing from the documentation provided. The absence of descriptive statistics raises concerns about the transparency of variable properties and the understanding of data distributions.

#### **Data Processing Rules**
While data processing (e.g., exclusion of night deposits, mapping transactions to forecast groups) is mentioned, there is **no formal documentation provided** that details how missing data or any backfilled data values were handled. The lack of empirical support for such decisions limits the ability to verify that data treatments were methodologically sound.

#### **Analyses Supporting Data Appropriateness**
Although the forecast is reviewed semi-annually by comparing actual and forecasted volumes, there is **no mention of specific analyses** supporting the appropriateness of the final model inputs. More detailed analysis should be documented to ensure that the chosen data inputs are empirically justified.

#### **Compliance with Regulatory Requirements**
There is **no explicit documentation** showing how the bankâ€™s data processing aligns with relevant regulatory data requirements. The absence of this documentation raises concerns about whether proper regulatory review has been conducted.

#### **Code Documentation**
There is **no mention** of the code used to process the source data into intermediate and final datasets, which is a critical aspect for reproducibility and validation.

### **Summary of Missing Elements**
- **Formal data dictionary** with variable descriptions, units, decoding rules, and descriptive statistics.
- **Detailed empirical support** for data cleaning and handling of missing or backfilled values.
- **Analysis supporting the appropriateness** of final model inputs.
- **Documentation of compliance** with regulatory requirements.
- **Code used** to process the data into final datasets. 

Without these elements, the model development process lacks transparency and robustness.
