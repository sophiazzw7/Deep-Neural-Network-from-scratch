## Model Validation Report for Loan Default Prediction Model

### 1. Introduction

**Objective:** This report aims to evaluate the performance of the loan default prediction model using the most recent OGM data (2024Q1). The analysis focuses on key performance metrics, comparing them to the development period, and providing critical insights and recommendations for improvement.

### 2. Dataset Overview

**Dataset:** The analysis is based on the OGM data from the first quarter of 2024. The dataset includes the following features:
- Application Type
- LOANSTATUS
- ACCOUNTSTATUS
- FUNDEDAMOUNT
- FUNDEDDATE
- LOANPURPOSE
- BEST_SCORE
- BEST_DECISIONZORS
- BEST_PLRS
- Target (default or not)

**Target Variable Distribution:**
- 0: 178,089 samples
- 1: 2,811 samples

### 3. Model Performance Metrics

#### 3.1. Funding Rate by Application Type

| Application Type | Funding Rate |
|------------------|--------------|
| Primary          | 98.80%       |
| Secondary        | 99.67%       |

#### 3.2. Funding Rate by Loan Purpose

| Loan Purpose          | Funding Rate |
|-----------------------|--------------|
| Auto                  | 99.16%       |
| Home Improvement      | 98.71%       |
| Debt Consolidation    | 98.93%       |
| Other                 | 98.91%       |

### 4. Model Evaluation Metrics

#### 4.1. ROC Curve and AUC

**ROC Curve:** The ROC curve plots the True Positive Rate (TPR) against the False Positive Rate (FPR) at various threshold settings. It helps in understanding the trade-off between sensitivity and specificity.

**AUC:** The Area Under the ROC Curve (AUC) quantifies the overall ability of the model to discriminate between positive and negative classes. An AUC value closer to 1 indicates better model performance.

- **ROC Curve Plot:**
  ```python
  from sklearn.metrics import roc_curve, auc

  fpr, tpr, _ = roc_curve(df['y_true'], df['y_score'])
  roc_auc = auc(fpr, tpr)

  plt.figure()
  plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
  plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
  plt.xlabel('False Positive Rate')
  plt.ylabel('True Positive Rate')
  plt.title('Receiver Operating Characteristic (ROC) Curve')
  plt.legend(loc="lower right")
  plt.show()
  ```

#### 4.2. KS Statistic

**KS Statistic:** The Kolmogorov-Smirnov (KS) statistic measures the maximum distance between the cumulative distribution functions of the positives and negatives. A higher KS statistic indicates better separation between positive and negative classes.

- **KS Statistic Calculation:**
  ```python
  ks_stat = max(tpr - fpr)
  print(f'KS Statistic: {ks_stat:.2f}')
  ```

#### 4.3. Gini Coefficient

**Gini Coefficient:** The Gini coefficient measures inequality among values in a frequency distribution and is derived from the AUC. It ranges from 0 to 1, with higher values indicating better performance.

- **Gini Coefficient Calculation:**
  ```python
  gini = 2 * roc_auc - 1
  print(f'Gini Coefficient: {gini:.2f}')
  ```

#### 4.4. Precision-Recall Curve

**Precision-Recall Curve:** This curve plots precision against recall for different threshold values. It is particularly useful for evaluating models on imbalanced datasets where positive instances are rare.

**Average Precision Score:** This score summarizes the precision-recall curve as the weighted mean of precisions achieved at each threshold.

- **Precision-Recall Curve Plot:**
  ```python
  from sklearn.metrics import precision_recall_curve, average_precision_score

  precision, recall, _ = precision_recall_curve(df['y_true'], df['y_score'])
  average_precision = average_precision_score(df['y_true'], df['y_score'])

  plt.figure()
  plt.plot(recall, precision, color='b', lw=2, label=f'PR curve (area = {average_precision:.2f})')
  plt.xlabel('Recall')
  plt.ylabel('Precision')
  plt.title('Precision-Recall Curve')
  plt.legend(loc="lower left")
  plt.show()
  ```

#### 4.5. Confusion Matrix and Derived Metrics

The confusion matrix provides a summary of prediction results, showing the counts of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN). From this matrix, various performance metrics can be derived:

| Metric     | Value         |
|------------|---------------|
| Accuracy   | (calculated)  |
| Precision  | (calculated)  |
| Recall     | (calculated)  |
| F1 Score   | (calculated)  |
| Specificity| (calculated)  |

**Note:** The metrics are calculated with a threshold set to 0.028 based on the distribution of `y_true` and `y_pred`.

- **Confusion Matrix and Metrics Calculation:**
  ```python
  from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score

  threshold = 0.028
  y_pred = (df['y_score'] >= threshold).astype(int)

  tn, fp, fn, tp = confusion_matrix(df['y_true'], y_pred).ravel()
  accuracy = accuracy_score(df['y_true'], y_pred)
  precision = precision_score(df['y_true'], y_pred, zero_division=1)
  recall = recall_score(df['y_true'], y_pred)
  f1 = f1_score(df['y_true'], y_pred)
  specificity = tn / (tn + fp) if (tn + fp) != 0 else 0

  metrics = {
      'Accuracy': accuracy,
      'Precision': precision,
      'Recall': recall,
      'F1 Score': f1,
      'Specificity': specificity
  }

  print(tabulate(metrics.items(), headers=['Metric', 'Value'], tablefmt='pretty'))
  ```

#### 4.6. Log Loss

**Log Loss:** Logarithmic Loss, or Log Loss, measures the performance of a classification model where the prediction output is a probability value between 0 and 1. Lower values of Log Loss indicate better model performance.

- **Log Loss Calculation:**
  ```python
  from sklearn.metrics import log_loss

  log_loss_value = log_loss(df['y_true'], df['y_score'])
  print(f'Log Loss: {log_loss_value:.2f}')
  ```

### 5. Visualizations

#### 5.1. Latency Analysis

**Latency Distribution:** This plot shows the distribution of latency values, helping identify any patterns or anomalies in processing times.

- **Latency Distribution Plot:**
  ```python
  plt.figure(figsize=(10, 6))
  sns.histplot(df['latency'], kde=True)
  plt.title('Latency Distribution')
  plt.xlabel('Latency')
  plt.ylabel('Frequency')
  plt.show()
  ```

**Correlation with Other Features:** The heatmap displays the correlation of latency with other key features.

- **Correlation Heatmap Plot:**
  ```python
  correlation_matrix = df[['latency', 'FUNDEDAMOUNT', 'approval date']].corr()

  plt.figure(figsize=(10, 6))
  sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
  plt.title('Correlation Matrix with Latency')
  plt.show()
  ```

#### 5.2. Approval and Funding Analysis

**Time to Fund Distribution:** This plot shows the distribution of the time taken to fund loans, providing insights into the efficiency of the funding process.

- **Time to Fund Distribution Plot:**
  ```python
  df['approval date'] = pd.to_datetime(df['approval date'])
  df['FUNDEDDATE'] = pd.to_datetime(df['FUNDEDDATE'])
  df['time_to_fund'] = (df['FUNDEDDATE'] - df['approval date']).dt.days

  plt.figure(figsize=(10, 6))
  sns.histplot(df['time_to_fund'].dropna(), kde=True)
  plt.title('Time to Fund Distribution')
  plt.xlabel('Days')
  plt.ylabel('Frequency')
  plt.show()
  ```

### 6. Insights and Recommendations

**Observations:**
- The funding rate is high across different application types and loan purposes, indicating robust model performance.
- Despite the high funding rates, the model's precision is significantly affected by the threshold setting, suggesting the need for fine-tuning.
- The low recall in an imbalanced dataset scenario indicates potential improvements in capturing true positives.

**Recommendations:**
1. **Threshold Adjustment:** Fine-tune the classification threshold to balance precision and recall effectively.
2. **Data Balancing:** Implement techniques such as SMOTE to handle imbalanced data, ensuring the model captures more positive samples.
3. **Model Retraining:** Periodically retrain the model with updated data to adapt to changing patterns and maintain performance.

### 7. Conclusion

The loan default prediction model demonstrates strong performance metrics, particularly in terms of funding rates. However, adjustments in threshold settings and handling data imbalance are crucial to enhancing precision and recall. Continuous monitoring and model retraining are recommended to ensure sustained performance.

---

This structure emphasizes independent testing and critical thinking, providing an effective

 challenge to model developers. By focusing on recent data and comparing performance metrics to the development period, you can offer valuable insights and actionable recommendations for model improvement.
