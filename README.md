Considering the risks of setting a monitoring baseline on potentially drifted performance, why was the average of the last four OGM quarters (2023Q2-2024Q1) chosen as the baseline instead of the model's initial implementation period performance on the Truist portfolio, which better represents expected performance before drift?'


1. The model currently has no Candidate Assessment form in SNOW. Could you please coordinate with Herman and Bobby to complete the MCA submission? 

3. The MDD notes that Model Development informed Truist that BDFS will be decommissioned and no longer available in 2026, and this model has one remaining OGM report before it is retired.
Could you please confirm the expected retirement timeline and describe what will follow after the model’s retirement — for example, whether a replacement or transition model is being planned,

Follow-ups on AH4950 (MCA, thresholds, retirement plan)

Hi [Name],

Quick follow-ups to support the current validation:

MCA in SNOW
It looks like there’s no Candidate Assessment on file. Could you coordinate with Herman/Bobby to submit the MCA? Also, the latest MCA appears outdated (e.g., it lists the model as internally built rather than vendor-sourced).

Monitoring baseline
Could you share the rationale for using the average of the last four OGM quarters (2023Q2–2024Q1) as the baseline? If feasible, we’d recommend anchoring thresholds to the initial implementation period on the Truist portfolio to better reflect pre-drift performance and improve defendability.

Post-retirement plan
The MDD notes one remaining OGM before retirement and BDFS decommissioning in 2026. Please confirm the expected retirement timeline and what follows afterward—replacement/transition model (if any), interim monitoring/reporting approach, and any dependencies that will be affected. This will help us decide the appropriate next steps (e.g., open issue vs. temporary monitoring control) in this validation.

Thanks so much—happy to discuss live if easier.

Best,
[Your Name]
