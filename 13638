**IMPACT ANALYSIS**

Prior attempts to forecast operational risk losses using quantitative methods—particularly negative binomial regression with macroeconomic variables—revealed weak or inconsistent linkages. Factors like fraud, litigation, and process breakdowns do not reliably correlate with broad economic indicators, so stress forecasts often required large manual overlays or haircuts. As a result, the institution’s modeling process became unnecessarily complex and still failed to capture the true, idiosyncratic nature of operational losses.

By shifting toward a qualitative multiplier-based approach focused on the most recent nine quarters of internal loss data, the model now relies on current, institution-specific conditions rather than tenuous macroeconomic correlations. This transition offers clearer alignment between observed loss patterns and forecasted scenarios, reducing the need for post-hoc adjustments. Although purely qualitative methods may forgo potential insights about economy-wide trends, the random, high-variance behavior of many operational loss events means these events are less likely to be systematically tied to economic cycles in the first place. Consequently, the new framework provides more realistic, timely estimates of operational risk exposure—reflecting the internal control environment, recent trends, and emerging threats—while acknowledging that the inherently idiosyncratic nature of operational risk remains challenging to predict with statistical precision.
 Removal of Negative Binomial Regression for EF and CPBP
EF and CPBP originally used a macro-driven framework (negative binomial regression) for frequency. However, correlations with economic variables often proved weak, causing large overlays to adjust model outputs. The new approach entirely removes macroeconomic factors for EF/CPBP, using a straightforward 9Q baseline frequency and a single stress multiplier (e.g., 3.5).
Adoption of Uniform Multiplier Logic for IF, EDPM, DPA, EPWS
The four other event types maintain a frequency-times-severity method that uses rolling 9Q ratios compared to a development average, with a 95th-percentile-based stress threshold. Both frequency and severity thus have explicit “non-stress” and “stress” multipliers derived from an internally consistent dataset.
